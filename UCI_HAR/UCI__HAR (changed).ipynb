{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "wzmW1dVfShYq",
        "outputId": "ef5ccda6-48ff-4c3f-b6c2-a5d561d3b15a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization) (1.22.4)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization) (1.15.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqmtLDdLwCZ3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.layers import Conv3D,Conv2D,Conv1D,MaxPooling1D, MaxPooling2D,TimeDistributed,LSTM,ConvLSTM2D, GRU\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import TimeDistributed\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF76KO27rXGJ"
      },
      "outputs": [],
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIzcRO99rXGK"
      },
      "outputs": [],
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        loaded.append(data)\n",
        "    # stack group so that features are the 3rd dimension\n",
        "    loaded = dstack(loaded)\n",
        "    return loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynbdZYEprXGK"
      },
      "outputs": [],
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    print('File Path : ',filepath)\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = load_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "    # load all train\n",
        "    trainX, trainy = load_dataset_group('train', prefix + '/content/gdrive/MyDrive/UCI HAR Dataset/')\n",
        "    # load all test\n",
        "    testX, testy = load_dataset_group('test', prefix + '/content/gdrive/MyDrive/UCI HAR Dataset/')\n",
        "    \n",
        "    #zero-offset class values\n",
        "    trainy = trainy - 1\n",
        "    testy = testy - 1\n",
        "    #one hot encode y\n",
        "    trainy_one_hot = to_categorical(trainy)\n",
        "    testy_one_hot = to_categorical(testy)\n",
        "    print(trainX.shape, trainy.shape, trainy_one_hot.shape, testX.shape, testy.shape, testy_one_hot.shape)\n",
        "    return trainX, trainy, trainy_one_hot, testX, testy, testy_one_hot"
      ],
      "metadata": {
        "id": "naEpkOKdp-ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX, trainy, trainy_one_hot, testX, testy, testy_one_hot = load_dataset()"
      ],
      "metadata": {
        "id": "GN4_D_nNqX5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_val,y_train_one_hot,y_val_one_hot,y_train,y_val=train_test_split(trainX, trainy_one_hot, trainy,test_size=0.3,random_state=100)"
      ],
      "metadata": {
        "id": "FBwxbhByqfVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, y_train_one_hot.shape, y_val_one_hot.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "id": "tVFIBuL2qiGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train_one_hot.shape[1]"
      ],
      "metadata": {
        "id": "Ibzr_TmQqkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_timesteps, n_features, n_outputs "
      ],
      "metadata": {
        "id": "iCTjlS6tqn3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN-LSTM**"
      ],
      "metadata": {
        "id": "dHiz6h76sOxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features)),\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    #tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.LSTM(units=64, activation='tanh', dropout=0.5),\n",
        "    tf.keras.layers.Dense(n_outputs, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JFjJRbR7f1Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "c1WzwXXPXKZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train_one_hot, validation_data=(X_val, y_val_one_hot), epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "E8MsSHsNXO8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_val, y_val_one_hot)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "pg_t91oTsbUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iFWFGtMQsiFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(history.history['accuracy'], 'r', label='Accuracy of training data')\n",
        "plt.plot(history.history['val_accuracy'], 'b', label='Accuracy of validation data')\n",
        "plt.plot(history.history['loss'], 'r--', label='Loss of training data')\n",
        "plt.plot(history.history['val_loss'], 'b--', label='Loss of validation data')\n",
        "plt.title('Model Accuracy and Loss')\n",
        "plt.ylabel('Accuracy and Loss')\n",
        "plt.xlabel('Training Epoch')\n",
        "plt.ylim(0)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sBhiM43Aqoz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase the size of the graphs. The default size is (6,4).\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "\n",
        "# graph the loss, the model above is configure to use \"mean squared error\" as the loss function\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'g.', linestyle='solid', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', linestyle='solid', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(plt.rcParams[\"figure.figsize\"])"
      ],
      "metadata": {
        "id": "fmqh-cn0W-EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph of mean absolute error\n",
        "mae = history.history['accuracy']\n",
        "val_mae = history.history['val_accuracy']\n",
        "plt.plot(epochs, mae, 'g.', linestyle='solid', label='Training Accuracy')\n",
        "plt.plot(epochs, val_mae, 'b.', linestyle='solid', label='Validation Accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4KxEdiOYsqln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion matrix**"
      ],
      "metadata": {
        "id": "QZIBbGRZP4wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(X_val)"
      ],
      "metadata": {
        "id": "4prp_uNGPw4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = tf.math.confusion_matrix(labels=tf.argmax(y_val_one_hot, 1), predictions=tf.argmax(predictions, 1))\n",
        "axis_labels = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n",
        "figure = plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True,cmap=plt.cm.Blues,xticklabels=axis_labels, yticklabels=axis_labels)\n",
        "plt.ylim(len(cm)-0, 0)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhZC4wd1P3aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "aoNfLseExhM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features)),\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    #tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(n_outputs, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "tF-m0em86-zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "MozdslgDllRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train_one_hot, validation_data=(X_val, y_val_one_hot), epochs=10, batch_size=32)"
      ],
      "metadata": {
        "id": "iF8KaIfxvj15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_val, y_val_one_hot)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "sIzVfqKIvpYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(history.history['accuracy'], 'r', label='Accuracy of training data')\n",
        "plt.plot(history.history['val_accuracy'], 'b', label='Accuracy of validation data')\n",
        "plt.plot(history.history['loss'], 'r--', label='Loss of training data')\n",
        "plt.plot(history.history['val_loss'], 'b--', label='Loss of validation data')\n",
        "plt.title('Model Accuracy and Loss')\n",
        "plt.ylabel('Accuracy and Loss')\n",
        "plt.xlabel('Training Epoch')\n",
        "plt.ylim(0)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FXL8jxw9Qbix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase the size of the graphs. The default size is (6,4).\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "\n",
        "# graph the loss, the model above is configure to use \"mean squared error\" as the loss function\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'g.', linestyle='solid', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', linestyle='solid', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(plt.rcParams[\"figure.figsize\"])"
      ],
      "metadata": {
        "id": "wEtPD2y6v0WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph of mean absolute error\n",
        "mae = history.history['accuracy']\n",
        "val_mae = history.history['val_accuracy']\n",
        "plt.plot(epochs, mae, 'g.', linestyle='solid', label='Training Accuracy')\n",
        "plt.plot(epochs, val_mae, 'b.', linestyle='solid', label='Validation Accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9IkzubNlv33m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "qcGlOcRKPQjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(X_val)"
      ],
      "metadata": {
        "id": "HWfJYjl3Najt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = tf.math.confusion_matrix(labels=tf.argmax(y_val_one_hot, 1), predictions=tf.argmax(predictions, 1))\n",
        "axis_labels = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n",
        "figure = plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True,cmap=plt.cm.Blues,xticklabels=axis_labels, yticklabels=axis_labels)\n",
        "plt.ylim(len(cm)-0, 0)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9RtSXuGtNaWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the TensorFlow Lite model into a C source file\n",
        "!xxd -i pruned_quantized_model.tflite > Pqmodel.cc\n",
        "\n",
        "# Create a header file from the C source file\n",
        "!sed 's/unsigned/const unsigned/g' Pqmodel.cc > Pqmodel.h\n",
        "\n",
        "import os\n",
        "Pqmodel_h_size = os.path.getsize(\"Pqmodel.h\")\n",
        "print(f\"Header file, Pqmodel.h, is {Pqmodel_h_size:,} bytes.\")"
      ],
      "metadata": {
        "id": "Ym2jXTFcnos_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Quantized Model"
      ],
      "metadata": {
        "id": "W628a1UC3in4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "  arr = []\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(X_val):\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    # print(test_image)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "\n",
        "  for i in range(0,len(y_val_one_hot)):\n",
        "    arr.append(np.argmax(y_val_one_hot[i]))\n",
        "  accuracy = (prediction_digits == np.array(arr)).mean()\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "vLEd4Zrd2mOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruned and Quantized model accuracy\n",
        "interpreter = tf.lite.Interpreter(model_content=pruned_quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "7aTJeptS4qAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruned model accuracy\n",
        "interpreter1 = tf.lite.Interpreter(model_content=tflite_pruned_model)\n",
        "interpreter1.allocate_tensors()\n",
        "\n",
        "test_accuracy_ = evaluate_model(interpreter1)\n",
        "print('Pruned TF test accuracy:', test_accuracy_)"
      ],
      "metadata": {
        "id": "noWvo49o3vYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MEx1rUy7kGgp"},"outputs":[],"source":["# Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZq9RBQ2kGgq"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6iH9S5EkGgr"},"outputs":[],"source":["# Activities are the class labels\n","# It is a 6 class classification\n","ACTIVITIES = {\n","    0: 'WALKING',\n","    1: 'WALKING_UPSTAIRS',\n","    2: 'WALKING_DOWNSTAIRS',\n","    3: 'SITTING',\n","    4: 'STANDING',\n","    5: 'LAYING',\n","}\n","\n","# Utility function to print the confusion matrix\n","def confusion_matrix(Y_true, Y_pred):\n","    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n","    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n","\n","    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txOKka-NlK2T","executionInfo":{"status":"ok","timestamp":1677228407696,"user_tz":-330,"elapsed":4248,"user":{"displayName":"shriniket dixit","userId":"13936259288715797272"}},"outputId":"872de2ca-b9e0-475b-a4db-2f59d1d6a3aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"g9PTOEX8kGgr"},"source":["### Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpbqwAJqkGgs"},"outputs":[],"source":["# Data directory\n","DATADIR = '/content/drive/MyDrive/uci-har dataset/UCI HAR Dataset'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhsGW5UIkGgs"},"outputs":[],"source":["# Raw data signals\n","# Signals are from Accelerometer and Gyroscope\n","# The signals are in x,y,z directions\n","# Sensor signals are filtered to have only body acceleration\n","# excluding the acceleration due to gravity\n","# Triaxial acceleration from the accelerometer is total acceleration\n","SIGNALS = [\n","    \"body_acc_x\",\n","    \"body_acc_y\",\n","    \"body_acc_z\",\n","    \"body_gyro_x\",\n","    \"body_gyro_y\",\n","    \"body_gyro_z\",\n","    \"total_acc_x\",\n","    \"total_acc_y\",\n","    \"total_acc_z\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgnZ_4YCkGgs"},"outputs":[],"source":["# Utility function to read the data from csv file\n","def _read_csv(filename):\n","    return pd.read_csv(filename, delim_whitespace=True, header=None)\n","\n","# Utility function to load the load\n","def load_signals(subset):\n","    signals_data = []\n","\n","    for signal in SIGNALS:\n","        filename = f'/content/drive/MyDrive/uci-har dataset/UCI HAR Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n","        signals_data.append(\n","            _read_csv(filename).as_matrix()\n","        ) \n","\n","    # Transpose is used to change the dimensionality of the output,\n","    # aggregating the signals by combination of sample/timestep.\n","    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n","    return np.transpose(signals_data, (1, 2, 0))\n","\n","def load_y(subset):\n","    \"\"\"\n","    The objective that we are trying to predict is a integer, from 1 to 6,\n","    that represents a human activity. We return a binary representation of \n","    every sample objective as a 6 bits vector using One Hot Encoding\n","    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n","    \"\"\"\n","    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n","    y = _read_csv(filename)[0]\n","\n","    return pd.get_dummies(y).as_matrix()\n","def load_data():\n","    \"\"\"\n","    Obtain the dataset from multiple files.\n","    Returns: X_train, X_test, y_train, y_test\n","    \"\"\"\n","    X_train, X_test = load_signals('train'), load_signals('test')\n","    y_train, y_test = load_y('train'), load_y('test')\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwOxqEDIkGgt"},"outputs":[],"source":["np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ha6rVYqakGgt"},"outputs":[],"source":["session_conf = tf.compat.v1.ConfigProto(\n","    intra_op_parallelism_threads=1,\n","    inter_op_parallelism_threads=1\n",")\n","\n","sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","tf.compat.v1.keras.backend.set_session(sess)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygzSz1gdkGgt"},"outputs":[],"source":["# Import Keras\n","from keras import backend as K\n","sess = tf.compat.v1.Session()\n","tf.compat.v1.keras.backend.set_session(sess)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"-6JyA2jXkGgu"},"outputs":[],"source":["# Importing libraries\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers.core import Dense, Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"jesE8vLGkGgu"},"outputs":[],"source":["# Initializing parameters\n","epochs = 30\n","batch_size = 16\n","n_hidden = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRoWuOu4kGgu"},"outputs":[],"source":["# Utility function to count the number of classes\n","def _count_classes(y):\n","    return len(set([tuple(category) for category in y]))"]},{"cell_type":"code","source":["# Loading the train and test data\n","X_train, X_test, Y_train, Y_test = load_data()\n","print(\"X train shape :\",X_train.shape)\n","print(\"X test shape :\",X_test.shape)\n","print(\"Y train shape :\",Y_train.shape)\n","print(\"Y train shape :\",Y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"u9qAf3aGq6n7","executionInfo":{"status":"error","timestamp":1677229555071,"user_tz":-330,"elapsed":1033,"user":{"displayName":"shriniket dixit","userId":"13936259288715797272"}},"outputId":"1dac5ccf-0bcb-4d9b-85b0-41208b8a6dd2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-22432d0a549f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X train shape :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X test shape :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Y train shape :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-72-55aab92e541d>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_signals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_signals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-e7efbab86250>\u001b[0m in \u001b[0;36mload_signals\u001b[0;34m(subset)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/content/drive/MyDrive/uci-har dataset/UCI HAR Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         signals_data.append(\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0m_read_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         ) \n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_matrix'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3dExrGjAq6q8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","def load_data():\n","    train_signals, train_labels = load_signals('train')\n","    test_signals, test_labels = load_signals('test')\n","    \n","    # Reshape the signal data into tensors\n","    train_signals = train_signals.reshape((train_signals.shape[0], train_signals.shape[1], 1))\n","    test_signals = test_signals.reshape((test_signals.shape[0], test_signals.shape[1], 1))\n","    \n","    # Convert the labels to one-hot encoding\n","    num_classes = np.max(train_labels) + 1\n","    train_labels = np.eye(num_classes)[train_labels]\n","    test_labels = np.eye(num_classes)[test_labels]\n","    \n","    # Convert the Pandas DataFrames to NumPy arrays\n","    X_train = train_signals.values\n","    X_test = test_signals.values\n","    Y_train = train_labels\n","    Y_test = test_labels\n","    \n","    return X_train, X_test, Y_train, Y_test\n"],"metadata":{"id":"Pca-1xiLmaUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_y(subset):\n","    \"\"\"\n","    The objective that we are trying to predict is a integer, from 1 to 6,\n","    that represents a human activity. We return a binary representation of \n","    every sample objective as a 6 bits vector using One Hot Encoding\n","    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n","    \"\"\"\n","    filename = f'{DATADIR}/{subset}/y_{subset}.txt'\n","    y = _read_csv(filename)[0]\n","\n","    return pd.get_dummies(y).to_numpy()"],"metadata":{"id":"BEn8ppLKmT2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ba3wi0XkGgu"},"outputs":[],"source":["def load_data():\n","    \"\"\"\n","    Obtain the dataset from multiple files.\n","    Returns: X_train, X_test, y_train, y_test\n","    \"\"\"\n","    X_train, X_test = load_signals('train'), load_signals('test')\n","    y_train, y_test = load_y('train'), load_y('test')\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","source":["import numpy as np\n","from pandas import read_csv\n","X_train = read_csv('/content/drive/MyDrive/uci-har dataset/UCI HAR Dataset/train/X_train.txt', header=None, delim_whitespace=True)\n","X_test = read_csv('/content/drive/MyDrive/uci-har dataset/UCI HAR Dataset/test/X_test.txt', header=None, delim_whitespace=True)\n","Y_train = read_csv('/content/drive/MyDrive/uci-har dataset/UCI HAR Dataset/train/y_train.txt', header=None, delim_whitespace=True)\n","Y_test = read_csv('/content/drive/MyDrive/uci-har dataset/UCI HAR Dataset/test/y_test.txt', header=None, delim_whitespace=True)\n","X_train = X_train.values\n","X_test = X_test.values\n","Y_train = Y_train.values\n","Y_test = Y_test.values\n","timesteps = X_train.shape[1]"],"metadata":{"id":"zJPZ5JKmnvHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_classes = _count_classes(Y_train)\n","print(\"X train shape: \", X_train.shape)\n","print(\"X test shape: \", X_test.shape)\n","print(\"Y train shape: \", Y_train.shape)\n","print(\"Y test shape: \", Y_test.shape)\n","print(\"Timesteps: \", timesteps)\n","print(\"Input dimensions: \", input_dim)\n","print(\"Number of classes: \", n_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG_XzlJSqakC","executionInfo":{"status":"ok","timestamp":1677229424148,"user_tz":-330,"elapsed":736,"user":{"displayName":"shriniket dixit","userId":"13936259288715797272"}},"outputId":"7f7ae5b8-e699-43c0-fc53-4b661523c75e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X train shape:  (7352, 561)\n","X test shape:  (2947, 561)\n","Y train shape:  (7352, 1)\n","Y test shape:  (2947, 1)\n","Timesteps:  561\n","Input dimensions:  561\n","Number of classes:  6\n"]}]},{"cell_type":"markdown","metadata":{"id":"e54LK3w7kGgv"},"source":["- Defining the Architecture of LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YNu2ZQRfkGgv","executionInfo":{"status":"ok","timestamp":1677229443284,"user_tz":-330,"elapsed":672,"user":{"displayName":"shriniket dixit","userId":"13936259288715797272"}},"outputId":"8a05552a-6e32-4572-ddf1-7d6dcfe1da15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 32)                76032     \n","                                                                 \n"," dropout (Dropout)           (None, 32)                0         \n","                                                                 \n"," dense (Dense)               (None, 6)                 198       \n","                                                                 \n","=================================================================\n","Total params: 76,230\n","Trainable params: 76,230\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Initiliazing the sequential model\n","model = Sequential()\n","# Configuring the parameters\n","model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n","# Adding a dropout layer\n","model.add(Dropout(0.5))\n","# Adding a dense output layer with sigmoid activation\n","model.add(Dense(n_classes, activation='sigmoid'))\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"g7HsZGh0kGgv"},"outputs":[],"source":["# Compiling the model\n","model.compile(loss='categorical_crossentropy',\n","              optimizer='rmsprop',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":990},"id":"E9zQktFhkGgv","executionInfo":{"status":"error","timestamp":1677229451663,"user_tz":-330,"elapsed":699,"user":{"displayName":"shriniket dixit","userId":"13936259288715797272"}},"outputId":"c914e973-a267-430b-e1d6-c253d7cc7781"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Model was constructed with shape (None, 561, 561) for input KerasTensor(type_spec=TensorSpec(shape=(None, 561, 561), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 561).\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-90-6957a225970c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit(X_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_1' (type Sequential).\n    \n    Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 561)\n    \n    Call arguments received by layer 'sequential_1' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 561), dtype=float32)\n      • training=True\n      • mask=None\n"]}],"source":["# Training the model\n","model.fit(X_train,\n","          Y_train,\n","          batch_size=batch_size,\n","          validation_data=(X_test, Y_test),\n","          epochs=epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWP8jqgEkGgv"},"outputs":[],"source":["# Confusion Matrix\n","print(confusion_matrix(Y_test, model.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoX2d9M5kGgv"},"outputs":[],"source":["score = model.evaluate(X_test, Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wXGs4pOkGgw"},"outputs":[],"source":["score"]},{"cell_type":"markdown","metadata":{"id":"OheZ_2ZmkGgw"},"source":["- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n","- We can further imporve the performace with Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"v0CrrvkikGgw"},"source":["### Assignment"]},{"cell_type":"markdown","metadata":{"id":"l3bDtHM1kGgw"},"source":["#### 2 LSTM Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvBOnSnEkGgw"},"outputs":[],"source":["# Importing libraries\n","from keras.models import Sequential\n","from keras.layers import LSTM,Dense,Dropout,Activation,BatchNormalization,Conv2D,Flatten,TimeDistributed,Conv1D\n","from keras.regularizers import *\n","from keras.callbacks import LearningRateScheduler,TerminateOnNaN,EarlyStopping\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import GridSearchCV\n","from keras.initializers import VarianceScaling\n","from keras.optimizers import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66a_kYKIkGgw"},"outputs":[],"source":["import math\n","def lr_decay(epoch):\n","    return float(0.0001 * math.pow(0.6, math.floor((1+epoch)/10)))\n","lr = LearningRateScheduler(lr_decay)\n","tm = TerminateOnNaN()\n","es = EarlyStopping(monitor = 'accuracy')\n","init = VarianceScaling(scale = 1.0,mode = 'fan_avg',distribution = 'normal')\n","adam = Adam(lr=0.001)\n","rmsprop = RMSprop(lr = 0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxzN0Iu2kGgw"},"outputs":[],"source":["\n","model = Sequential()\n","model.add(LSTM(32,activation = 'relu',return_sequences=True, input_shape=(timesteps, input_dim),recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.003) ))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.6))\n","model.add(LSTM(32,recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.003)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.4))\n","model.add(Dense(6, activation='sigmoid'))\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKhFnksFkGgx"},"outputs":[],"source":["%%time\n","model.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1Rv1DaokGgx"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model.fit(X_train,\n","          Y_train,\n","          batch_size=32,\n","          validation_data=(X_test, Y_test),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCqzgq1nkGgx"},"outputs":[],"source":["model.evaluate(X_test,Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Qjm0JfJkGgx"},"outputs":[],"source":["# Confusion Matrix\n","print(confusion_matrix(Y_test, model.predict(X_test)))"]},{"cell_type":"markdown","metadata":{"id":"HNKu_OOTkGgx"},"source":["### 3 LSTM Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBz3MjFkkGgx"},"outputs":[],"source":["model = Sequential()\n","model.add(LSTM(16,activation = 'relu',return_sequences=True, input_shape=(timesteps, input_dim),recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.003) ))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.6))\n","model.add(LSTM(16,return_sequences=True,recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.003)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.4))\n","model.add(LSTM(16,recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.003)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.4))\n","model.add(Dense(6, activation='sigmoid'))\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slCSu2dLkGgx"},"outputs":[],"source":["%%time\n","model.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_RjSa5HkGgy"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model.fit(X_train,\n","          Y_train,\n","          batch_size=32,\n","          validation_data=(X_test, Y_test),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fySrv8mlkGgy"},"outputs":[],"source":["model.evaluate(X_test,Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"apDH2syskGgy"},"outputs":[],"source":["# Confusion Matrix\n","print(confusion_matrix(Y_test, model.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ERprfmLkGgy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2pNkKKLBkGgy"},"outputs":[],"source":["model = Sequential()\n","model.add(LSTM(100,activation = 'relu',return_sequences=True, input_shape=(timesteps, input_dim),recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.003) ))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.8))\n","model.add(LSTM(100,recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.0003)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.8))\n","# model.add(LSTM(28,return_sequences=True,recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.0003)))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.8))\n","# # model.add(LSTM(64,return_sequences=True,recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.003)))\n","# # model.add(BatchNormalization())\n","# # model.add(Dropout(0.6))\n","# model.add(LSTM(16,recurrent_initializer=\"glorot_uniform\",recurrent_regularizer=l2(0.0003)))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.8))\n","model.add(Dense(n_classes, activation='softmax'))\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOeV-melkGgy"},"outputs":[],"source":["%%time\n","model.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btSXQKv1kGgz"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model.fit(X_train,\n","          Y_train,\n","          batch_size=32,\n","          validation_data=(X_test, Y_test),callbacks=[lr,tm],\n","          epochs=50,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvTJKOttkGgz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcTaRAFmkGgz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RD9mFFq0kGgz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"vPXCkV8VkGgz"},"source":["### Using CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqYPMMnzkGgz"},"outputs":[],"source":["# Importing libraries\n","from keras.models import Sequential\n","from keras.layers import LSTM,Dense,Dropout,Activation,BatchNormalization,Conv2D,Flatten,TimeDistributed,Conv1D,MaxPool1D\n","from keras.regularizers import *\n","from keras.callbacks import LearningRateScheduler,TerminateOnNaN\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import GridSearchCV\n","from keras.initializers import VarianceScaling\n","from keras.optimizers import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LYlz2STkGgz"},"outputs":[],"source":["import math\n","def lr_decay(epoch):\n","    return float(0.001 * math.pow(0.6, math.floor((1+epoch)/10)))\n","lr = LearningRateScheduler(lr_decay)\n","tm = TerminateOnNaN()\n","init = VarianceScaling(scale = 1.0,mode = 'fan_avg',distribution = 'normal')\n","adam = Adam(lr=0.001)\n","rmsprop = RMSprop(lr = 0.001)"]},{"cell_type":"markdown","metadata":{"id":"AFC_RKh8kGgz"},"source":["#### Model 1 CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWPRlNbPkGgz"},"outputs":[],"source":["model1 = Sequential()\n","model1.add(Conv1D(128,kernel_size=3,kernel_initializer='he_normal',input_shape=(timesteps, input_dim),kernel_regularizer = l2(0.003),activation ='relu'))\n","model1.add(BatchNormalization())\n","model1.add(Dropout(0.7))\n","model1.add(MaxPool1D(2))\n","# model1.add(Conv1D(64,kernel_size=3,kernel_initializer='he_normal',kernel_regularizer = l2(0.003),activation ='relu'))\n","# model1.add(BatchNormalization())\n","# model1.add(Dropout(0.5))\n","model1.add(Conv1D(32,kernel_size=3,kernel_initializer='he_normal',kernel_regularizer = l2(0.003),activation ='relu'))\n","model1.add(BatchNormalization())\n","model1.add(Dropout(0.5))\n","model1.add(MaxPool1D(2))\n","model1.add(Flatten())\n","model1.add(Dense(64,activation = 'relu'))\n","model1.add(Dropout(0.5))\n","model1.add(Dense(n_classes,activation = 'softmax'))\n","model1.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqr-xELbkGgz"},"outputs":[],"source":["%%time\n","model1.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXK5_TIpkGg0"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model1.fit(X_train,\n","          Y_train,\n","          batch_size=32,\n","          validation_data=(X_test, Y_test),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcUp3TzskGg0"},"outputs":[],"source":["model1.evaluate(X_test,Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gurHudMEkGg0"},"outputs":[],"source":["# Confusion Matrix\n","print(confusion_matrix(Y_test, model1.predict(X_test)))"]},{"cell_type":"markdown","metadata":{"id":"2_D7kfaXkGg0"},"source":["#### Model2 CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk4z_mChkGg0"},"outputs":[],"source":["model2 = Sequential()\n","model2.add(Conv1D(128,kernel_size=7,kernel_initializer='he_normal',input_shape=(timesteps, input_dim),kernel_regularizer = l2(0.003),activation ='relu'))\n","model2.add(BatchNormalization())\n","model2.add(Dropout(0.7))\n","model2.add(MaxPool1D(2))\n","# model2.add(Conv1D(64,kernel_size=3,kernel_initializer='he_normal',kernel_regularizer = l2(0.003),activation ='relu'))\n","# model2.add(BatchNormalization())\n","# model2.add(Dropout(0.5))\n","model2.add(Conv1D(64,kernel_size=7,kernel_initializer='he_normal',kernel_regularizer = l2(0.003),activation ='relu'))\n","model2.add(BatchNormalization())\n","model2.add(Dropout(0.7))\n","model2.add(MaxPool1D(2))\n","model2.add(Flatten())\n","model2.add(Dense(32,activation = 'relu'))\n","model2.add(Dropout(0.5))\n","model2.add(Dense(n_classes,activation = 'softmax'))\n","model2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"temK9TDPkGg0"},"outputs":[],"source":["%%time\n","model2.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3OE9uQiukGg0"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model2.fit(X_train,\n","          Y_train,\n","          batch_size=64,\n","          validation_data=(X_test, Y_test),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPU0vzCHkGg1"},"outputs":[],"source":["model2.evaluate(X_test,Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szIL-3gFkGg1"},"outputs":[],"source":["# Confusion Matrix\n","print(confusion_matrix(Y_test, model2.predict(X_test)))"]},{"cell_type":"markdown","metadata":{"id":"GMTmT1qGkGg1"},"source":["#### Using Divide and Conquer Based Approach"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wtHoPJ1kGg1"},"outputs":[],"source":["# Utility function to read the data from csv file\n","def _read_csv(filename):\n","    return pd.read_csv(filename, delim_whitespace=True, header=None)\n","\n","# Utility function to load the load\n","def load_signals(subset):\n","    signals_data = []\n","\n","    for signal in SIGNALS:\n","        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n","        signals_data.append(\n","            _read_csv(filename).as_matrix()\n","        ) \n","\n","    # Transpose is used to change the dimensionality of the output,\n","    # aggregating the signals by combination of sample/timestep.\n","    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n","    return np.transpose(signals_data, (1, 2, 0))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JB2SoUoNkGg1"},"outputs":[],"source":["def load_y_bi(subset):\n","    \"\"\"\n","    The objective that we are trying to predict is a integer, from 1 to 6,\n","    that represents a human activity. We return a binary representation of \n","    every sample objective as a 6 bits vector using One Hot Encoding\n","    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n","    \"\"\"\n","    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n","    y = _read_csv(filename)[0]\n","    y[y<=3] = 0\n","    y[y>3] = 1\n","    return pd.get_dummies(y).as_matrix()\n","\n","def load_data_bi():\n","    \"\"\"\n","    Obtain the dataset from multiple files.\n","    Returns: X_train, X_test, y_train, y_test\n","    \"\"\"\n","    X_train, X_test = load_signals('train'), load_signals('test')\n","    y_train,y_test = load_y_bi('train'),load_y_bi(\"test\")\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9d12b23kGg1"},"outputs":[],"source":["X_train_bi,X_test_bi,Y_train_bi,Y_test_bi  = load_data_bi()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Tdsmfz8kGg2"},"outputs":[],"source":["model_bi= Sequential()\n","model_bi.add(Conv1D(32,kernel_size=3,kernel_initializer='he_normal',input_shape=(timesteps, input_dim),kernel_regularizer = l2(0.003),activation ='relu'))\n","#model_bi.add(BatchNormalization())\n","#model_bi.add(MaxPool1D(2))\n","model_bi.add(Conv1D(32,kernel_size=3,kernel_initializer='he_normal',kernel_regularizer = l2(0.003),activation ='relu'))\n","#model_bi.add(BatchNormalization())\n","model_bi.add(Dropout(0.6))\n","model_bi.add(MaxPool1D(2))\n","model_bi.add(Flatten())\n","model_bi.add(Dense(64,activation = 'relu'))\n","# model_bi.add(Dropout(0.5))\n","model_bi.add(Dense(2,activation = 'softmax'))\n","model_bi.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJgVwlTXkGg2"},"outputs":[],"source":["%%time\n","model_bi.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWHwiS9KkGg2"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model_bi.fit(X_train_bi,\n","          Y_train_bi,\n","          batch_size=64,\n","          validation_data=(X_test_bi, Y_test_bi),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ao1tQ8KpkGg2"},"outputs":[],"source":["model_bi.evaluate(X_test_bi,Y_test_bi)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7n1KD2RnkGg2"},"outputs":[],"source":["# Confusion Matrix\n","print(confusion_matrix(Y_test, model_bi.predict(X_test_bi)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zl-_zfQOkGg2"},"outputs":[],"source":["model_bi.save(\"model_bi_class.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou4qgqwAkGg2"},"outputs":[],"source":["def load_y_new(subset):\n","    \"\"\"\n","    The objective that we are trying to predict is a integer, from 1 to 6,\n","    that represents a human activity. We return a binary representation of \n","    every sample objective as a 6 bits vector using One Hot Encoding\n","    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n","    \"\"\"\n","    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n","    y = _read_csv(filename)[0]\n","    label_y = y>3\n","    new_y = y[label_y]\n","    return pd.get_dummies(new_y).as_matrix(),label_y\n","\n","def load_data_new():\n","    \"\"\"\n","    Obtain the dataset from multiple files.\n","    Returns: X_train, X_test, y_train, y_test\n","    \"\"\"\n","    X_train, X_test = load_signals('train'), load_signals('test')\n","    y_train_new, y_label_train = load_y_new('train')\n","    y_val_new,y_label_test = load_y_new('test')\n","    X_train_new = X_train[y_label_train]\n","    X_val_new = X_test[y_label_test]\n","    return X_train_new, X_val_new, y_train_new, y_val_new"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Es_es422kGg2"},"outputs":[],"source":["X_train_st,X_test_st,Y_train_st,Y_test_st  = load_data_new()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85x30rldkGg3"},"outputs":[],"source":["model_st= Sequential()\n","model_st.add(Conv1D(30,kernel_size=3,kernel_initializer='glorot_normal',input_shape=(timesteps, input_dim),kernel_regularizer = l2(0.0003),activation ='relu'))\n","model_st.add(BatchNormalization())\n","model_st.add(Dropout(0.7))\n","# model_st.add(MaxPool1D(2))\n","model_st.add(Conv1D(50,kernel_size=3,kernel_initializer='glorot_normal',kernel_regularizer = l2(0.0003),activation ='relu'))\n","model_st.add(BatchNormalization())\n","model_st.add(Dropout(0.4))\n","model_st.add(Conv1D(100,kernel_size=3,kernel_initializer='glorot_normal',kernel_regularizer = l2(0.0003),activation ='relu'))\n","model_st.add(BatchNormalization())\n","model_st.add(Dropout(0.4))\n","#model_st.add(MaxPool1D(2))\n","model_st.add(Flatten())\n","model_st.add(Dense(100,activation = 'relu'))\n","model_st.add(Dropout(0.6))\n","model_st.add(Dense(3,activation = 'softmax'))\n","model_st.summary()\n","# model_st = Sequential()\n","# model_st.add(Conv1D(100,kernel_size=3,kernel_initializer='he_normal',input_shape=(timesteps, input_dim),kernel_regularizer = l2(0.0003),activation ='relu'))\n","# model_st.add(BatchNormalization())\n","# model_st.add(Dropout(0.7))\n","# # model_st.add(MaxPool1D(2))\n","# model_st.add(Conv1D(100,kernel_size=3,kernel_initializer='he_normal',kernel_regularizer = l2(0.0003),activation ='relu'))\n","# model_st.add(BatchNormalization())\n","# model_st.add(Dropout(0.7))\n","# model_st.add(Conv1D(500,kernel_size=3,kernel_initializer='he_normal',kernel_regularizer = l2(0.0003),activation ='relu'))\n","# model_st.add(BatchNormalization())\n","# model_st.add(Dropout(0.7))\n","# #model_st.add(MaxPool1D(2))\n","# model_st.add(Flatten())\n","# model_st.add(Dense(32,activation = 'relu'))\n","# model_st.add(Dropout(0.2))\n","# model_st.add(Dense(3,activation = 'softmax'))\n","# model_st.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kt0iJNIPkGg3"},"outputs":[],"source":["%%time\n","model_st.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrgT9JvSkGg3"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model_st.fit(X_train_st,\n","          Y_train_st,\n","          batch_size=64,\n","          validation_data=(X_test_st, Y_test_st),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaZyB3T4kGg3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5pjYDcAEkGg3"},"outputs":[],"source":["# Confusion Matrix\n","#print(confusion_matrix(np.argmax(Y_test_st,axis=1), np.argmax(model_st.predict(X_test_st),axis=1)))\n","model_st.evaluate(X_test_st,Y_test_st)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RBqoQbekGg3"},"outputs":[],"source":["print(confusion_matrix(np.argmax(Y_test_st,axis=1), np.argmax(model_st.predict(X_test_st),axis=1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JbGXeaskGg3"},"outputs":[],"source":["model_st.save(\"model_st_class.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meGvHm9OkGg3"},"outputs":[],"source":["def load_y_new(subset):\n","    \"\"\"\n","    The objective that we are trying to predict is a integer, from 1 to 6,\n","    that represents a human activity. We return a binary representation of \n","    every sample objective as a 6 bits vector using One Hot Encoding\n","    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n","    \"\"\"\n","    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n","    y = _read_csv(filename)[0]\n","    label_y = y<=3\n","    new_y = y[label_y]\n","    return pd.get_dummies(new_y).as_matrix(),label_y\n","\n","def load_data_new():\n","    \"\"\"\n","    Obtain the dataset from multiple files.\n","    Returns: X_train, X_test, y_train, y_test\n","    \"\"\"\n","    X_train, X_test = load_signals('train'), load_signals('test')\n","    y_train_new, y_label_train = load_y_new('train')\n","    y_val_new,y_label_test = load_y_new('test')\n","    X_train_new = X_train[y_label_train]\n","    X_val_new = X_test[y_label_test]\n","    return X_train_new, X_val_new, y_train_new, y_val_new"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsFruDZMkGg4"},"outputs":[],"source":["X_train_dy,X_test_dy,Y_train_dy,Y_test_dy  = load_data_new()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zWmMKP8kGg4"},"outputs":[],"source":["model_dy= Sequential()\n","model_dy.add(Conv1D(64,kernel_size=5,kernel_initializer='glorot_normal',input_shape=(timesteps, input_dim),kernel_regularizer = l2(0.0003),activation ='relu'))\n","#model_dy.add(BatchNormalization())\n","#model_dy.add(MaxPool1D(2))\n","model_dy.add(Conv1D(32,kernel_size=5,kernel_initializer='glorot_normal',kernel_regularizer = l2(0.0003),activation ='relu'))\n","#model_dy.add(BatchNormalization())\n","model_dy.add(Dropout(0.7))\n","# model_dy.add(Conv1D(32,kernel_size=7,kernel_initializer='he_normal',kernel_regularizer = l2(0.003),activation ='relu'))\n","# #model_dy.add(BatchNormalization())\n","# model_dy.add(Dropout(0.6))\n","model_dy.add(MaxPool1D(2))\n","model_dy.add(Flatten())\n","model_dy.add(Dense(16,activation = 'relu'))\n","model_dy.add(Dropout(0.2))\n","model_dy.add(Dense(3,activation = 'softmax'))\n","model_dy.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tekB_7DxkGg4"},"outputs":[],"source":["%%time\n","model_dy.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgNAJ51GkGg4"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model_dy.fit(X_train_dy,\n","          Y_train_dy,\n","          batch_size=64,\n","          validation_data=(X_test_dy, Y_test_dy),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92hOkzENkGg4"},"outputs":[],"source":["# Confusion Matrix\n","#print(confusion_matrix(Y_test_dy, model_dy.predict(X_test_dy)))\n","model_dy.evaluate(X_test_dy,Y_test_dy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvzvidbxkGg4"},"outputs":[],"source":["model_dy.save(\"model_dy_class.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqeaOgSskGg4"},"outputs":[],"source":["model_dy.load_weights(\"model_dy_class.h5\")"]},{"cell_type":"markdown","metadata":{"id":"7XGtGxP6kGg4"},"source":["#### Sharpening Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmeYBMWPkGg5"},"outputs":[],"source":["def sharpen(x_test, sigma, alpha):\n","    d = x_test.shape[0]\n","    r = x_test.shape[1]\n","    c = x_test.shape[2]\n","    container = np.empty((d,r, c))\n","    i = 0\n","\n","    for row in x_test:\n","        test = row\n","        blurred = ndimage.gaussian_filter(test, sigma)\n","        sharpened = test + alpha * (test - blurred)\n","        container[i] = sharpened\n","        i = i + 1\n","    return container"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMQLy5yGkGg5"},"outputs":[],"source":["from scipy import ndimage\n","alpha = np.arange(0.05, 2.55, 0.05)\n","sigma = np.arange(5, 10, 1)\n","from sklearn.metrics import accuracy_score,confusion_matrix\n","def display_output(X_test,Y_test):\n","    accuracy=[]\n","    for s in sigma:\n","        for a in alpha:\n","            # Sharpen test data with various sigma (for Gaussian filter) and alpha value combinations\n","            X_test_sharpen = sharpen(X_test, s, a)\n","            pred_dyna_sharpen = model_dy.predict(X_test_sharpen)\n","            accuracy.append(accuracy_score(Y_test, np.argmax(pred_dyna_sharpen, axis=1)))\n","    return(accuracy)\n","            #print(\">>> sigma={}, alpha={:.2f}\".format(s, a))\n","#             print(accuracy_score(np.argmax(Y_test,axis=1), np.argmax(pred_dyna_sharpen, axis=1)))\n","#             print(confusion_matrix(np.argmax(Y_test,axis=1), np.argmax(pred_dyna_sharpen, axis=1)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIx9vxfskGg5"},"outputs":[],"source":["# Utility function to read the data from csv file\n","def _read_csv(filename):\n","    return pd.read_csv(filename, delim_whitespace=True, header=None)\n","\n","# Utility function to load the load\n","def load_signals(subset):\n","    signals_data = []\n","\n","    for signal in SIGNALS:\n","        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n","        signals_data.append(\n","            _read_csv(filename).as_matrix()\n","        ) \n","\n","    return np.transpose(signals_data, (1, 2, 0))\n","\n","def load_y(subset):\n","    \"\"\"\n","    The objective that we are trying to predict is a integer, from 1 to 6,\n","    that represents a human activity. We return a binary representation of \n","    every sample objective as a 6 bits vector using One Hot Encoding\n","    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n","    \"\"\"\n","    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n","    y = _read_csv(filename)[0]\n","\n","    return pd.get_dummies(y).as_matrix()\n","def load_data():\n","    \"\"\"\n","    Obtain the dataset from multiple files.\n","    Returns: X_train, X_test, y_train, y_test\n","    \"\"\"\n","    X_train, X_test = load_signals('train'), load_signals('test')\n","    y_train, y_test = load_y('train'), load_y('test')\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMkF5hD5kGg5"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","X_train, X_test, Y_train, Y_test = load_data()\n","print(\"X train shape :\",X_train.shape)\n","print(\"X test shape :\",X_test.shape)\n","print(\"Y train shape :\",Y_train.shape)\n","print(\"Y train shape :\",Y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"WKvetd_fkGg5"},"source":["### Classification of Sitting and Laying"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCil4TtXkGg5"},"outputs":[],"source":["X_train_st_sitlay = X_train_st[np.argmax(Y_train_st,axis=1)!=1]\n","X_test_st_sitlay = X_test_st[np.argmax(Y_test_st,axis=1)!=1]\n","Y_train_st_sitlay = Y_train_st[np.argmax(Y_train_st,axis=1)!=1]\n","Y_test_st_sitlay = Y_test_st[np.argmax(Y_test_st,axis=1)!=1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHkOWdAGkGg5"},"outputs":[],"source":["model_st_bi= Sequential()\n","model_st_bi.add(Conv1D(64,kernel_size=5,kernel_initializer='glorot_normal',input_shape=(timesteps, input_dim),kernel_regularizer = l2(0.0003),activation ='relu'))\n","#model_st_bi.add(BatchNormalization())\n","#model_st_bi.add(MaxPool1D(2))\n","model_st_bi.add(Conv1D(32,kernel_size=5,kernel_initializer='glorot_normal',kernel_regularizer = l2(0.0003),activation ='relu'))\n","#model_st_bi.add(BatchNormalization())\n","model_st_bi.add(Dropout(0.7))\n","# model_st_bi.add(Conv1D(32,kernel_size=7,kernel_initializer='he_normal',kernel_regularizer = l2(0.003),activation ='relu'))\n","# #model_st_bi.add(BatchNormalization())\n","# model_st_bi.add(Dropout(0.6))\n","model_st_bi.add(MaxPool1D(2))\n","model_st_bi.add(Flatten())\n","model_st_bi.add(Dense(16,activation = 'relu'))\n","model_st_bi.add(Dropout(0.3))\n","model_st_bi.add(Dense(3,activation = 'softmax'))\n","model_st_bi.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cTxzfA5kGg5"},"outputs":[],"source":["%%time\n","model_st_bi.compile(loss='categorical_crossentropy',\n","          optimizer=adam,\n","          metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mThD7iokGg6"},"outputs":[],"source":["%%time\n","# Training the model\n","result = model_st_bi.fit(X_train_st_sitlay,\n","          Y_train_st_sitlay,\n","          batch_size=64,\n","          validation_data=(X_test_st_sitlay, Y_test_st_sitlay),callbacks=[lr,tm],\n","          epochs=30,verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykq0FO3dkGg6"},"outputs":[],"source":["print(confusion_matrix(np.argmax(Y_test_st_sitlay,axis=1), np.argmax(model_st_bi.predict(X_test_st_sitlay),axis=1)))\n","model_st_bi.evaluate(X_test_st_sitlay,Y_test_st_sitlay)"]},{"cell_type":"markdown","metadata":{"id":"v4iT0kqBkGg6"},"source":["### Best Sigma and alpha for Test Sharpening"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHKrJukykGg6"},"outputs":[],"source":["accuracy=[]\n","alpha = np.arange(0.05, 2.55, 0.05)\n","sigma = np.arange(0, 5, 0.5) \n","for s in sigma:\n","    for a in alpha:\n","        X_test_st_ = sharpen(X_test_st_,s,a)\n","        X_test_dy_ = sharpen(X_test_dy_,s,a)\n","        y_pred_st = model_st.predict_classes(X_test_st_)+3\n","        y_pred_dy = model_dy.predict_classes(X_test_dy_)\n","        total_y = np.concatenate([y_pred_st,y_pred_dy])\n","        accuracy.append(accuracy_score(np.argmax(y_test,axis=1),total_y))"]},{"cell_type":"markdown","metadata":{"id":"Cqm7h5apkGg6"},"source":["### Stacking the models for Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bc0BjrkqkGg6"},"outputs":[],"source":["y_pred_bi = model_bi.predict_classes(X_test_bi)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mX91GwoCkGg6"},"outputs":[],"source":["X_test_st_= X_test[y_pred_bi>0]   # Static class\n","X_test_dy_ = X_test[y_pred_bi<1]  # Dynamic Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sB_mPXwlkGg6"},"outputs":[],"source":["Y_test_st_ = Y_test[y_pred_bi>0]\n","Y_test_dy_ = Y_test[y_pred_bi<1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4GdHPp5kGg6"},"outputs":[],"source":["y_test = np.concatenate([Y_test_st_,Y_test_dy_]) #"]},{"cell_type":"markdown","metadata":{"id":"0ccDIUu3kGg7"},"source":["### Sharpening of Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Nar3-I6kGg7"},"outputs":[],"source":["X_test_st_ = sharpen(X_test_st_,5,0.05)\n","X_test_dy_ = sharpen(X_test_dy_,5,0.05)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXRZOUHkkGg7"},"outputs":[],"source":["y_pred_st = model_st.predict_classes(X_test_st_)+3\n","y_pred_dy = model_dy.predict_classes(X_test_dy_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvGaFwC9kGg7"},"outputs":[],"source":["total_y = np.concatenate([y_pred_st,y_pred_dy])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ps3M2mlfkGg7"},"outputs":[],"source":["print(\"Accuracy Using Divide and Conquer Method->\",accuracy_score(np.argmax(y_test,axis=1),total_y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ed1rv0ozkGg7"},"outputs":[],"source":["import seaborn as sns\n","sns.heatmap(confusion_matrix(np.argmax(y_test,axis=1),total_y),annot=True)"]},{"cell_type":"markdown","metadata":{"id":"bu8X7ueikGg7"},"source":["### Conclusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT0OP-jhkGg7"},"outputs":[],"source":["from prettytable import PrettyTable\n","x = PrettyTable()\n","x.field_names = [\"Model\",\"Accuracy\",\"loss\"]\n","x.add_row([\"LSTM with one Layer\",90.97,0.3087])\n","x.add_row([\"LSTM with two Layer\",91.78,0.3971])\n","x.add_row([\"LSTM with three Layer\",89.51,0.4863])\n","x.add_row([\"Divide-Conquer Method\",94.24,0.2042])\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"2ZUzvW6OkGg8"},"source":["1. All three model is getting confused between standing and sitting, after running so many code at last I got 91.8% accuracy in LSTM with 2 layers.\n","2. Some code gives nan as my losses and get stuck in same accuracy of 16.83% for many epochs ."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}
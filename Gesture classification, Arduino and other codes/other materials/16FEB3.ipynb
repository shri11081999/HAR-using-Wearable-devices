{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328b8182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_csv\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flatten\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\__init__.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_source_inputs\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_summary\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_to_dot\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\vis_utils.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wrapper\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# `pydot` is an optional dependency,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# see `extras_require` in `setup.py`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# note: `Node` is an internal class,\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# it isn't meant to be used by Keras users.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputLayer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputSpec\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_layer.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Layer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m count_params\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_arg\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\initializers\\__init__.py:130\u001b[0m\n\u001b[0;32m    124\u001b[0m     LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# For backwards compatibility, we populate this file with the objects\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# from ALL_OBJECTS. We make no guarantees as to whether these objects will\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# using their correct version.\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[43mpopulate_deserializable_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Utility functions\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\initializers\\__init__.py:87\u001b[0m, in \u001b[0;36mpopulate_deserializable_objects\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m v2_objs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     86\u001b[0m base_cls \u001b[38;5;241m=\u001b[39m initializers_v2\u001b[38;5;241m.\u001b[39mInitializer\n\u001b[1;32m---> 87\u001b[0m \u001b[43mgeneric_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate_dict_with_module_objects\u001b[49m(\n\u001b[0;32m     88\u001b[0m     v2_objs,\n\u001b[0;32m     89\u001b[0m     [initializers_v2],\n\u001b[0;32m     90\u001b[0m     obj_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: inspect\u001b[38;5;241m.\u001b[39misclass(x) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x, base_cls),\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m v2_objs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     93\u001b[0m     LOCAL\u001b[38;5;241m.\u001b[39mALL_OBJECTS[key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b837d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    print('File Path : ',filepath)\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all train\n",
    "X_train, Y_train = load_dataset_group('train', 'C:/Users/dixit/Desktop/uci-har dataset/UCI HAR Dataset/')\n",
    "# load all test\n",
    "X_test, Y_test = load_dataset_group('test', 'C:/Users/dixit/Desktop/uci-har dataset/UCI HAR Dataset/')\n",
    "\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "# one hot encode y\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004237b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"har_weights.h5\", monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), \n",
    "                    epochs=epochs, batch_size=batch_size, callbacks = [checkpoint], verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, Y_test, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - 1D CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57bfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for training data\n",
    "y_pred_train = model.predict(X_train)\n",
    "# Take the class with the highest probability from the train predictions\n",
    "max_y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "#print(classification_report(trainy, max_y_pred_train))\n",
    "\n",
    "# confusion matrix\n",
    "LABELS = ['WALKING',\n",
    "          'WALKING_UPSTAIRS',\n",
    "          'WALKING_DOWNSTAIRS',\n",
    "          'SITTING',\n",
    "          'STANDING',\n",
    "          'LAYING']\n",
    "y_pred_test = model.predict(X_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "matrix = metrics.confusion_matrix(max_y_test, max_y_pred_test)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(matrix,\n",
    "            cmap='coolwarm',\n",
    "            linecolor='white',\n",
    "            linewidths=1,\n",
    "            xticklabels=LABELS,\n",
    "            yticklabels=LABELS,\n",
    "            annot=True,\n",
    "            fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D, LSTM\n",
    "from keras.layers import Permute, Reshape\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d76dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7630df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    print('File Path : ',filepath)\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all train\n",
    "X_train, Y_train = load_dataset_group('train', 'C:/Users/dixit/Desktop/uci-har dataset/UCI HAR Dataset/')\n",
    "# load all test\n",
    "X_test, Y_test = load_dataset_group('test', 'C:/Users/dixit/Desktop/uci-har dataset/UCI HAR Dataset/')\n",
    "\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "# one hot encode y\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]\n",
    "\n",
    "# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "n_steps, n_length = 4, 32\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], n_steps, 1, n_length, n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], n_steps, 1, n_length, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b594a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "hist = model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test), batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586574f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for training data\n",
    "y_pred_train = model.predict(X_train)\n",
    "# Take the class with the highest probability from the train predictions\n",
    "max_y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "#print(classification_report(trainy, max_y_pred_train))\n",
    "\n",
    "# confusion matrix\n",
    "LABELS = ['WALKING',\n",
    "          'WALKING_UPSTAIRS',\n",
    "          'WALKING_DOWNSTAIRS',\n",
    "          'SITTING',\n",
    "          'STANDING',\n",
    "          'LAYING']\n",
    "y_pred_test = model.predict(X_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "matrix = metrics.confusion_matrix(max_y_test, max_y_pred_test)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(matrix,\n",
    "            cmap='PiYG_r',\n",
    "            linecolor='white',\n",
    "            linewidths=1,\n",
    "            xticklabels=LABELS,\n",
    "            yticklabels=LABELS,\n",
    "            annot=True,\n",
    "            fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe433d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

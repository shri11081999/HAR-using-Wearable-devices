{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOT09SgFlI272+VaMLPfTu3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"NFFLrodEnaoi","executionInfo":{"status":"error","timestamp":1677195401312,"user_tz":-330,"elapsed":3099,"user":{"displayName":"shriniket dixit","userId":"13936259288715797272"}},"outputId":"7f8dd923-3370-4580-c015-d4754a04aef4"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-37e16a93616d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional_recurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvLSTM2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.convolutional_recurrent'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Importing Libraries\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","from keras.layers import Conv2D, MaxPooling2D, Conv1D, Flatten\n","from keras.layers.convolutional import Conv3D\n","from keras.layers.convolutional_recurrent import ConvLSTM2D\n","from keras.layers.normalization import BatchNormalization\n","\n","# Importing libraries\n","from keras.models import Sequential\n","from keras.layers import LSTM, TimeDistributed\n","from keras.layers.core import Dense, Dropout"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","# get the features from the file features.txt\n","features = list()\n","with open('UCI_HAR_Dataset/features.txt') as f:\n","    features = [line.split()[1] for line in f.readlines()]\n","print('No of Features: {}'.format(len(features)))"],"metadata":{"id":"D_HduNlIqNwl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the data from txt files to pandas dataffame\n","X_train = pd.read_csv('UCI_HAR_dataset/train/X_train.txt', delim_whitespace=True, header=None, names=features)\n","\n","# add subject column to the dataframe\n","X_train['subject'] = pd.read_csv('UCI_HAR_dataset/train/subject_train.txt', header=None, squeeze=True)\n","\n","y_train = pd.read_csv('UCI_HAR_dataset/train/y_train.txt', names=['Activity'], squeeze=True)\n","y_train_labels = y_train.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n","                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n","\n","# put all columns in a single dataframe\n","train = X_train\n","train['Activity'] = y_train\n","train['ActivityName'] = y_train_labels\n","train.sample()"],"metadata":{"id":"5hVDgVyvqPdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.shape"],"metadata":{"id":"5NCXseEiqPg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the data from txt files to pandas dataffame\n","X_test = pd.read_csv('UCI_HAR_dataset/test/X_test.txt', delim_whitespace=True, header=None, names=features)\n","\n","# add subject column to the dataframe\n","X_test['subject'] = pd.read_csv('UCI_HAR_dataset/test/subject_test.txt', header=None, squeeze=True)\n","\n","# get y labels from the txt file\n","y_test = pd.read_csv('UCI_HAR_dataset/test/y_test.txt', names=['Activity'], squeeze=True)\n","y_test_labels = y_test.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n","                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n","\n","\n","# put all columns in a single dataframe\n","test = X_test\n","test['Activity'] = y_test\n","test['ActivityName'] = y_test_labels\n","test.sample()"],"metadata":{"id":"3FwbpBvlqPje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test.shape"],"metadata":{"id":"JvA8HQdoqPmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('No of duplicates in train: {}'.format(sum(train.duplicated())))\n","print('No of duplicates in test : {}'.format(sum(test.duplicated())))"],"metadata":{"id":"MS3QstEKqPpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('We have {} NaN/Null values in train'.format(train.isnull().values.sum()))\n","print('We have {} NaN/Null values in test'.format(test.isnull().values.sum()))"],"metadata":{"id":"QR9_mqPLqPsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set_style('whitegrid')\n","plt.rcParams['font.family'] = 'Dejavu Sans'"],"metadata":{"id":"q70nGfhEqPvE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(16,8))\n","plt.title('Data provided by each user', fontsize=20)\n","sns.countplot(x='subject',hue='ActivityName', data = train)\n","plt.show()"],"metadata":{"id":"Ap_bl7avqPx7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.title('No of Datapoints per Activity', fontsize=15)\n","sns.countplot(train.ActivityName)\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"8Rm82w96qNz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["columns = train.columns\n","\n","# Removing '()' from column names\n","columns = columns.str.replace('[()]','')\n","columns = columns.str.replace('[-]', '')\n","columns = columns.str.replace('[,]','')\n","\n","train.columns = columns\n","test.columns = columns\n","\n","test.columns"],"metadata":{"id":"pIpO-m3oqo8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.to_csv('UCI_HAR_Dataset/csv_files/train.csv', index=False)\n","test.to_csv('UCI_HAR_Dataset/csv_files/test.csv', index=False)"],"metadata":{"id":"1XkIAfrPqo_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set_palette(\"Set1\", desat=0.80)\n","facetgrid = sns.FacetGrid(train, hue='ActivityName', size=6,aspect=2)\n","facetgrid.map(sns.distplot,'tBodyAccMagmean', hist=False)\\\n","    .add_legend()\n","plt.annotate(\"Stationary Activities\", xy=(-0.956,17), xytext=(-0.9, 23), size=20,\\\n","            va='center', ha='left',\\\n","            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n","\n","plt.annotate(\"Moving Activities\", xy=(0,3), xytext=(0.2, 9), size=20,\\\n","            va='center', ha='left',\\\n","            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n","plt.show()"],"metadata":{"id":"ln643YcWqpC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for plotting purposes taking datapoints of each activity to a different dataframe\n","df1 = train[train['Activity']==1]\n","df2 = train[train['Activity']==2]\n","df3 = train[train['Activity']==3]\n","df4 = train[train['Activity']==4]\n","df5 = train[train['Activity']==5]\n","df6 = train[train['Activity']==6]\n","\n","plt.figure(figsize=(14,7))\n","plt.subplot(2,2,1)\n","plt.title('Stationary Activities(Zoomed in)')\n","sns.distplot(df4['tBodyAccMagmean'],color = 'r',hist = False, label = 'Sitting')\n","sns.distplot(df5['tBodyAccMagmean'],color = 'm',hist = False,label = 'Standing')\n","sns.distplot(df6['tBodyAccMagmean'],color = 'c',hist = False, label = 'Laying')\n","plt.axis([-1.01, -0.5, 0, 35])\n","plt.legend(loc='center')\n","\n","plt.subplot(2,2,2)\n","plt.title('Moving Activities')\n","sns.distplot(df1['tBodyAccMagmean'],color = 'red',hist = False, label = 'Walking')\n","sns.distplot(df2['tBodyAccMagmean'],color = 'blue',hist = False,label = 'Walking Up')\n","sns.distplot(df3['tBodyAccMagmean'],color = 'green',hist = False, label = 'Walking down')\n","plt.legend(loc='center right')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"KdDSY6OHqpFv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(7,7))\n","sns.boxplot(x='ActivityName', y='tBodyAccMagmean',data=train, showfliers=False, saturation=1)\n","plt.ylabel('Acceleration Magnitude mean')\n","plt.axhline(y=-0.7, xmin=0.1, xmax=0.9,dashes=(5,5), c='g')\n","plt.axhline(y=-0.05, xmin=0.4, dashes=(5,5), c='m')\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"M9WVb1qnqpIo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.boxplot(x='ActivityName', y='angleXgravityMean', data=train)\n","plt.axhline(y=0.08, xmin=0.1, xmax=0.9,c='m',dashes=(5,3))\n","plt.title('Angle between X-axis and Gravity_mean', fontsize=15)\n","plt.xticks(rotation = 40)\n","plt.show()"],"metadata":{"id":"9XT_IOw3qpLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.boxplot(x='ActivityName', y='angleYgravityMean', data = train, showfliers=False)\n","plt.title('Angle between Y-axis and Gravity_mean', fontsize=15)\n","plt.xticks(rotation = 40)\n","plt.axhline(y=-0.22, xmin=0.1, xmax=0.8, dashes=(5,3), c='m')\n","plt.show()"],"metadata":{"id":"ApJokG0Mqw5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"X1p9GMEtqw8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# performs t-sne with different perplexity values and their repective plots..\n","\n","def perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n","        \n","    for index,perplexity in enumerate(perplexities):\n","        # perform t-sne\n","        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n","        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n","        print('Done..')\n","        \n","        # prepare the data for seaborn         \n","        print('Creating plot for this t-sne visualization..')\n","        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n","        \n","        # draw the plot in appropriate place in the grid\n","        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\\\n","                   palette=\"Set1\",markers=['^','v','s','o', '1','2'])\n","        plt.title(\"perplexity : {} and max_iter : {}\".format(perplexity, n_iter))\n","        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n","        print('saving this plot as image in present working directory...')\n","        plt.savefig(img_name)\n","        plt.show()\n","        print('Done')"],"metadata":{"id":"KC_Z6U1Hqw_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_pre_tsne = train.drop(['subject', 'Activity','ActivityName'], axis=1)\n","y_pre_tsne = train['ActivityName']\n","perform_tsne(X_data = X_pre_tsne,y_data=y_pre_tsne, perplexities =[2,5,10,20,50])"],"metadata":{"id":"OhekpBA6qxCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yeJAp2TGrBhf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"K_JOAN9DqxFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('UCI_HAR_dataset/csv_files/train.csv')\n","test = pd.read_csv('UCI_HAR_dataset/csv_files/test.csv')\n","print(train.shape, test.shape)"],"metadata":{"id":"TJgu1aZbqxIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.head(3)"],"metadata":{"id":"I73rt7NHqxLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get X_train and y_train from csv files\n","X_train = train.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n","y_train = train.ActivityName"],"metadata":{"id":"moH6bIEqqpOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get X_test and y_test from test csv file\n","X_test = test.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n","y_test = test.ActivityName"],"metadata":{"id":"UosxWm4qqN3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Activities are the class labels\n","# It is a 6 class classification\n","ACTIVITIES = {\n","    0: 'WALKING',\n","    1: 'WALKING_UPSTAIRS',\n","    2: 'WALKING_DOWNSTAIRS',\n","    3: 'SITTING',\n","    4: 'STANDING',\n","    5: 'LAYING',\n","}\n","\n","# Utility function to print the confusion matrix\n","def confusion_matrix(Y_true, Y_pred):\n","    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n","    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n","\n","    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"],"metadata":{"id":"a1XzAHhsorkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data directory\n","DATADIR = 'UCI_HAR_Dataset'"],"metadata":{"id":"aGl0DV0Co0GE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Raw data signals\n","# Signals are from Accelerometer and Gyroscope\n","# The signals are in x,y,z directions\n","# Sensor signals are filtered to have only body acceleration\n","# excluding the acceleration due to gravity\n","# Triaxial acceleration from the accelerometer is total acceleration\n","SIGNALS = [\n","    \"body_acc_x\",\n","    \"body_acc_y\",\n","    \"body_acc_z\",\n","    \"body_gyro_x\",\n","    \"body_gyro_y\",\n","    \"body_gyro_z\",\n","    \"total_acc_x\",\n","    \"total_acc_y\",\n","    \"total_acc_z\"\n","]"],"metadata":{"id":"E-Wqrlrso0JQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utility function to read the data from csv file\n","def _read_csv(filename):\n","    return pd.read_csv(filename, delim_whitespace=True, header=None)\n","\n","def load_signals(subset):\n","    signals_data = []\n","\n","    for signal in SIGNALS:\n","        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n","        signals_data.append(\n","            _read_csv(filename).as_matrix()\n","        ) \n","\n","    # Transpose is used to change the dimensionality of the output,\n","    # aggregating the signals by combination of sample/timestep.\n","    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n","    return np.transpose(signals_data, (1, 2, 0))"],"metadata":{"id":"yk_gH_iHo0Mg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_y(subset):\n","    \"\"\"\n","    The objective that we are trying to predict is a integer, from 1 to 6,\n","    that represents a human activity. We return a binary representation of \n","    every sample objective as a 6 bits vector using One Hot Encoding\n","    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n","    \"\"\"\n","    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n","    y = _read_csv(filename)[0]\n","\n","    return pd.get_dummies(y).as_matrix()"],"metadata":{"id":"r9t7pNgno0Ps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data():\n","    \"\"\"\n","    Obtain the dataset from multiple files.\n","    Returns: X_train, X_test, y_train, y_test\n","    \"\"\"\n","    X_train, X_test = load_signals('train'), load_signals('test')\n","    y_train, y_test = load_y('train'), load_y('test')\n","\n","    return X_train, X_test, y_train, y_test"],"metadata":{"id":"Z-4Vfz0jo0Su"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing tensorflow\n","np.random.seed(42)\n","import tensorflow as tf\n","tf.set_random_seed(42)"],"metadata":{"id":"4G9mlfG3o0Y_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configuring a session\n","session_conf = tf.ConfigProto(\n","    intra_op_parallelism_threads=1,\n","    inter_op_parallelism_threads=1\n",")"],"metadata":{"id":"wuUs8133pP3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import Keras\n","from keras import backend as K\n","sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n","K.set_session(sess)"],"metadata":{"id":"EjbkUdAKpP61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing parameters\n","epochs = 30\n","batch_size = 30\n","# n_hidden = 32"],"metadata":{"id":"Ym1wp12ipP9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utility function to count the number of classes\n","def _count_classes(y):\n","    return len(set([tuple(category) for category in y]))"],"metadata":{"id":"Ij6oZTuNpQAy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.datasets import make_classification\n","from keras.models import load_model"],"metadata":{"id":"zJ_oG5L7pQDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the train and test data\n","X_train, X_test, Y_train, Y_test = load_data()"],"metadata":{"id":"9FlqQEUVpQGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# selecting axis specific data\n","X_train_1 = X_train[:,:,[1,2,3,4,5,6]]\n","X_test_1 = X_test[:,:,[1,2,3,4,5,6]]"],"metadata":{"id":"1neIJX0epQJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# selecting axis specific data\n","X_train_2 = X_train[:,:,[1,2,3,4]]\n","X_test_2 = X_test[:,:,[1,2,3,4]]"],"metadata":{"id":"t2v9PFDbpQM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# X_train_2 = X_train[:,:,[5,6,7,8]]\n","# X_test_2 = X_test[:,:,[5,6,7,8]]"],"metadata":{"id":"QNZLnArwpQP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # selecting axis specific data\n","# X_train_1 = X_train[:,:,[3,4,5,6,7,8]]\n","# X_train_2 = X_train[:,:,[3,4,6,7]]"],"metadata":{"id":"SKEJ80BtpQTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # selecting axis specific data\n","# X_test_1 = X_test[:,:,[3,4,5,6,7,8]]\n","# X_test_2 = X_test[:,:,[3,4,6,7]]"],"metadata":{"id":"M6g7mYmTpQWO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["timesteps = len(X_train[0])\n","input_dim = len(X_train[0][0])\n","n_classes = _count_classes(Y_train)\n","print(timesteps)\n","print(input_dim)\n","print(len(X_train))\n","num_classes = 6"],"metadata":{"id":"5ghR0stppQZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test and Train data reshaping for 1st dataset\n","# input image dimensions\n","\n","img_rows, img_cols = 128, 9\n","# an activity is governed by sequence of activities. 8 sequences informations are given to CNN model which will be later given to LSTM unit as a sequence information.\n","# 7352 = 919*8\n","X_train = X_train.reshape(919,8,128,9,1)\n","Y_train = Y_train.reshape(919,8,6)\n","\n","# removing last 3 data pointjust to make test data sequence compatible\n","# 2944 = 368*8\n","X_test = X_test[:-3]\n","Y_test = Y_test[:-3]\n","\n","X_test = X_test.reshape(368,8,128,9,1)\n","Y_test = Y_test.reshape(368,8,6)\n","\n","# Input shape for model-1\n","input_shape_1 = ( X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4])\n","print(input_shape_1)"],"metadata":{"id":"VsUvY_w8pQcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape, X_test.shape"],"metadata":{"id":"6IBt8XKFpQfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test and Train data reshaping for 1st dataset\n","# input image dimensions\n","\n","img_rows, img_cols = 128, 6\n","# an activity is governed by sequence of activities. 8 sequences informations are given to CNN model which will be later given to LSTM unit as a sequence information.\n","# 7352 = 919*8\n","X_train_1 = X_train_1.reshape(919,8,128,6,1)\n","\n","# removing last 3 data pointjust to make test data sequence compatible\n","# 2944 = 368*8\n","X_test_1 = X_test_1[:-3]\n","X_test_1 = X_test_1.reshape(368,8,128,6,1)\n","\n","# Input shape for model-2\n","input_shape_2 = (X_train_1.shape[1], X_train_1.shape[2], X_train_1.shape[3], X_train_1.shape[4])\n","print(input_shape_2)"],"metadata":{"id":"rEOxYlggpkcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_1.shape, X_test_1.shape"],"metadata":{"id":"9A5V1BVNpkfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test and Train data reshaping for 1st dataset\n","# input image dimensions\n","\n","img_rows, img_cols = 128, 4\n","# an activity is governed by sequence of activities. 8 sequences are given to CNN model which will be later given to LSTM unit as a sequence information.\n","# 7352 = 919*8\n","X_train_2 = X_train_2.reshape(919,8,128,4,1)\n","\n","# removing last 3 data pointjust to make test data sequence compatible\n","# 2944 = 368*8\n","X_test_2 = X_test_2[:-3]\n","X_test_2 = X_test_2.reshape(368,8,128,4,1)\n","\n","# Input shape for model-2\n","input_shape_3 = (X_train_2.shape[1], X_train_2.shape[2], X_train_2.shape[3], X_train_2.shape[4])\n","print(input_shape_3)"],"metadata":{"id":"xYsyPRzkpkiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_2.shape, X_test_2.shape"],"metadata":{"id":"wK5HA3Hapklz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_model(shape_cnn, shape_lstm):\n","    # print(shape_cnn, shape_lstm)\n","    model = Sequential()\n","    model.add(TimeDistributed(Conv2D(128, kernel_size=(5,1),  activation='relu', input_shape= shape_cnn)))\n","    model.add(TimeDistributed(Conv2D(64, (5, 1), activation='relu')))\n","    model.add(Dropout(0.5))\n","    model.add(TimeDistributed(Conv2D(32, (5, 1), activation='relu')))\n","    model.add(TimeDistributed(Flatten()))\n","    # model.add(TimeDistributed(Dense(32, activation='relu')))\n","    model.add(LSTM(units=64, return_sequences=True, input_shape = shape_lstm))\n","    model.add(Dense(num_classes, activation='softmax'))\n","    # compiling the model\n","    # model.summary()\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model"],"metadata":{"id":"8E0Pwh5Opko8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1 = get_model(input_shape_1,(input_shape_1[1],input_shape_1[2]))\n","model_1.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1)"],"metadata":{"id":"WcjeZGWrpksE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_1.save('CNN_LSTM_Model_1.h5')"],"metadata":{"id":"HOFl0UY-pkvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = model_1.evaluate(X_test, Y_test)"],"metadata":{"id":"EvrE34yGpkym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score"],"metadata":{"id":"uO9tnXS-o0cO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2 = get_model(input_shape_2,(input_shape_2[1],input_shape_2[2]))\n","model_2.fit(X_train_1, Y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n","model_2.save('CNN_LSTM_Model_2.h5')"],"metadata":{"id":"EtmdDQuao0fi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = model_2.evaluate(X_test_1, Y_test)"],"metadata":{"id":"nrHizU9io0jb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score"],"metadata":{"id":"IZZX-KHQo0nC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3 = get_model(input_shape_3,(input_shape_3[1],input_shape_3[2]))\n","model_3.fit(X_train_2, Y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n","model_3.save('CNN_LSTM_Model_3.h5')"],"metadata":{"id":"d6G6vneEo0qR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = model_3.evaluate(X_test_2, Y_test)"],"metadata":{"id":"27tQF8M4pzJV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score"],"metadata":{"id":"5FLm4BfipzMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test = Y_test.reshape(2944,6)\n","\n","y1 = model_1.predict_proba(X_test)\n","y1 = y1.reshape(2944,6)\n","\n","y2 = model_2.predict_proba(X_test_1)\n","y2 = y2.reshape(2944,6)\n","\n","y3 = model_3.predict_proba(X_test_2)\n","y3 = y3.reshape(2944,6)"],"metadata":{"id":"EDA-vF8ApzPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = []\n","for i in range(len(y1)):\n","    tmp = [0,0,0,0,0,0]\n","    \n","    op1 = np.amax(y1[i])\n","    index1 = y1[i].argmax(axis=0)\n","    tmp[index1] = 1\n","    \n","    op2 = np.amax(y2[i])\n","    index2 = y2[i].argmax(axis=0)\n","    tmp[index2] = 1\n","    \n","    op3 = np.amax(y3[i])\n","    index3 = y3[i].argmax(axis=0)\n","    tmp[index3] = 1\n","    \n","    l = [op1, op2, op3]\n","    if sum(tmp)==1:\n","        y_pred.append(tmp)\n","    \n","    else:\n","        tmp = [0,0,0,0,0,0]\n","        ind = l.index(max(l))\n","        if ind == 0:\n","            tmp[index1] = 1\n","            y_pred.append(tmp)\n","        \n","        elif ind ==1:\n","            tmp[index2] = 1\n","            y_pred.append(tmp)\n","        \n","        elif ind ==2:\n","            tmp[index3] = 1\n","            y_pred.append(tmp)"],"metadata":{"id":"rT8d0WhFpzSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion Matrix\n","y_pred = np.array(y_pred)\n","print(confusion_matrix(y_test, y_pred))"],"metadata":{"id":"G9JKoiFNpzVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score"],"metadata":{"id":"r6P4BpvVpzYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy_score(y_test, y_pred)"],"metadata":{"id":"1jVqdEYIpzbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create function returning a compiled network\n","def create_network(n_hidden, drop):\n","    \n","    # Initiliazing the sequential model\n","    model = Sequential()\n","    \n","    # Configuring the parameters\n","    model.add(LSTM(units=n_hidden, dropout=drop, return_sequences=True, input_shape=(timesteps, input_dim)))\n","    model.add(LSTM(units=n_hidden, dropout=drop, return_sequences=False))\n","\n","    # Adding a dropout layer\n","    model.add(Dropout(drop))\n","    # Adding a dense output layer with sigmoid activation\n","    model.add(Dense(n_classes, activation='sigmoid'))\n","    \n","    # Compile model\n","    model.compile(loss='categorical_crossentropy',\n","              optimizer='rmsprop',\n","              metrics=['accuracy'])\n","    # model.summary()\n","\n","    return model"],"metadata":{"id":"fTnwDvU3pzec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Wrap Keras model so it can be used by scikit-learn\n","neural_network = KerasClassifier(build_fn=create_network, verbose=0, epochs=epochs, batch_size=batch_size)"],"metadata":{"id":"XZxOcx6EpzhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create grid search\n","grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters, verbose=10, cv=2)\n","\n","# Fit grid search\n","grid_result = grid.fit(X_train, Y_train)\n","grid_result.best_params_"],"metadata":{"id":"e5EJgvTTpzj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initializing parameters\n","epochs = 20\n","batch_size = 50\n","# n_hidden = 32"],"metadata":{"id":"ITdeR2HjqABt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_hidden = 264\n","drop= 0.7\n","# Initiliazing the sequential model\n","model = Sequential()\n","# Configuring the parameters\n","model.add(LSTM(units = n_hidden, return_sequences=True, input_shape=(timesteps, input_dim)))\n","model.add(Dropout(drop))\n","model.add(LSTM(units =  n_hidden, return_sequences=False))\n","# Adding a dropout layer\n","model.add(Dropout(drop))\n","# Adding a dense output layer with sigmoid activation\n","model.add(Dense(n_classes, activation='sigmoid'))\n","model.summary()"],"metadata":{"id":"AwJ6vysBqAE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compiling the model\n","model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"],"metadata":{"id":"8FSmSwIxqAHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training the model\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs)"],"metadata":{"id":"s2GFqSS2qAKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion Matrix\n","print(confusion_matrix(Y_test, model1.predict(X_test)))"],"metadata":{"id":"PGf6eLaEqANi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = model1.evaluate(X_test, Y_test)"],"metadata":{"id":"MYVvqVAgqGqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score"],"metadata":{"id":"JD_jeoFxqGtK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from prettytable import PrettyTable    \n","x = PrettyTable()\n","x.field_names = [\"Architecture\", \"Test Accuracy\"]\n","x.add_row([\"DNN Fusion\", \"0.95\"])\n","x.add_row([\"LSTM\", \"0.90\"])\n","\n","print(x)"],"metadata":{"id":"IDnJu7YVqGwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5wYUCobHqGy6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JTJvGCLWqG2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PzbnM8TaqG44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cf2UQL76qG7g"},"execution_count":null,"outputs":[]}]}